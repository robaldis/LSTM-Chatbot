{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import torch\n",
    "from torch.jit import script, trace\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import codecs\n",
    "from io import open\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "# Makes sure to use cuda if there is a graphics card that it can be run on\n",
    "# speeds up the the training of the model\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Data processing\n",
    "The first step is to understand and clean the data that is being used.\n",
    "This consits of reading the data and saving it into a csv file that is in the right format with word pairs that can be used to train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "b'L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\\n'\nb'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!\\n'\nb'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.\\n'\nb'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?\\n'\nb\"L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\\n\"\nb'L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow\\n'\nb\"L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie.\\n\"\nb'L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No\\n'\nb'L870 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I\\'m kidding.  You know how sometimes you just become this \"persona\"?  And you don\\'t know how to quit?\\n'\nb'L869 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Like my fear of wearing pastels?\\n'\n"
     ]
    }
   ],
   "source": [
    "# Understanding what format the downloaded file is in\n",
    "corpus_name = \"cornell_movie-dialogs_corpus\"\n",
    "corpus = os.path.join(\"assets\", corpus_name)\n",
    "\n",
    "def printLines(file, n=10):\n",
    "    with open(file, 'rb') as datafile:\n",
    "        lines = datafile.readlines()\n",
    "    for line in lines[:n]:\n",
    "        print(line)\n",
    "\n",
    "printLines(os.path.join(corpus, \"movie_lines.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits line of the file into a dictionary of fileds.\n",
    "def loadLines (fileName, fields):\n",
    "    lines = {}\n",
    "    with open(fileName, 'r', encoding='iso-8859-1') as f:\n",
    "        for line in f:\n",
    "            values = line.split(\" +++$+++ \")\n",
    "            # Extract fields\n",
    "            lineObj = {}\n",
    "            for i, field in enumerate(fields):\n",
    "                lineObj[field] = values[i]\n",
    "            lines[lineObj['lineID']] = lineObj\n",
    "    return lines\n",
    "\n",
    "\n",
    "# Groups fields of lines from 'loadLines' into conversations based on *movie_conversations.txt*\n",
    "def loadConversations (fileName, lines, fields):\n",
    "    conversations =  []\n",
    "    with open (fileName, 'r', encoding='iso-8859-1') as f:\n",
    "        for line in f:\n",
    "            values = line.split(\" +++$+++ \")\n",
    "            # Extrac fields\n",
    "            convObj = {}\n",
    "            for i, field in enumerate(fields):\n",
    "                convObj[field] = values[i]\n",
    "            # Conver string to list (convObj[\"utteranceIDs\"] == \"['L54354', 'L54345', ...]\")\n",
    "            utterance_id_pattern = re.compile('L[0-9]+')\n",
    "            lineIds = utterance_id_pattern.findall(convObj[\"utteranceIDs\"])\n",
    "            # Reassemble lines\n",
    "            convObj[\"lines\"] = []\n",
    "            for lineId in lineIds:\n",
    "                convObj[\"lines\"].append(lines[lineId])#\n",
    "            conversations.append(convObj)\n",
    "    return conversations\n",
    "\n",
    "\n",
    "# Extracts pairs of sentences from conversatiosn\n",
    "def extractSentencePairs(conversations):\n",
    "    qa_pairs = []\n",
    "    for conversation in conversations:\n",
    "        # Iterate over all the lines of the cconversation\n",
    "        for i in range(len(conversation[\"lines\"]) -1 ): # Ignore the last line (no answer for it)\n",
    "            inputLine = conversation[\"lines\"][i][\"text\"].strip()\n",
    "            targetLine = conversation[\"lines\"][i+1][\"text\"].strip()\n",
    "            # Filter wrong sample (if on e of the lists is empty)\n",
    "            if inputLine and targetLine:\n",
    "                qa_pairs.append([inputLine, targetLine])\n",
    "    return qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Processing corpus...\n",
      "\n",
      "Loading conversations...\n",
      "\n",
      "Writing newly formattd file...\n",
      "\n",
      "Sample lines from file:\n",
      "b\"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\tWell, I thought we'd start with pronunciation, if that's okay with you.\\n\"\n",
      "b\"Well, I thought we'd start with pronunciation, if that's okay with you.\\tNot the hacking and gagging and spitting part.  Please.\\n\"\n",
      "b\"Not the hacking and gagging and spitting part.  Please.\\tOkay... then how 'bout we try out some French cuisine.  Saturday?  Night?\\n\"\n",
      "b\"You're asking me out.  That's so cute. What's your name again?\\tForget it.\\n\"\n",
      "b\"No, no, it's my fault -- we didn't have a proper introduction ---\\tCameron.\\n\"\n",
      "b\"Cameron.\\tThe thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\n\"\n",
      "b\"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\tSeems like she could get a date easy enough...\\n\"\n",
      "b'Why?\\tUnsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\\n'\n",
      "b\"Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\\tThat's a shame.\\n\"\n",
      "b'Gosh, if only we could find Kat a boyfriend...\\tLet me see what I can do.\\n'\n"
     ]
    }
   ],
   "source": [
    "# Define path to new file\n",
    "datafile = os.path.join(corpus, \"formatted_movie_lines.txt\")\n",
    "\n",
    "delimiter = '\\t'\n",
    "# Unescape the delimiter\n",
    "delimiter = str(codecs.decode(delimiter, \"unicode_escape\"))\n",
    "\n",
    "# Initialise lines dict, conversations list, and field ids\n",
    "lines = {}\n",
    "CONVERSATIONS = []\n",
    "MOVIE_LINES_FIELDS = [\"lineID\", \"characterID\", \"movieID\", \"character\", \"text\"]\n",
    "MOVIE_CONVERSATIONS_FIELDS = [\"character1ID\", \"character2ID\", \"movieID\", \"utteranceIDs\"]\n",
    "\n",
    "# Load lines and process conversations\n",
    "print(\"\\nProcessing corpus...\")\n",
    "lines = loadLines(os.path.join(corpus, \"movie_lines.txt\"), MOVIE_LINES_FIELDS)\n",
    "print(\"\\nLoading conversations...\")\n",
    "conversations = loadConversations(os.path.join(corpus, \"movie_conversations.txt\"), lines, MOVIE_CONVERSATIONS_FIELDS)\n",
    "\n",
    "# Write new csv file\n",
    "print(\"\\nWriting newly formattd file...\")\n",
    "with open(datafile, 'w', encoding='utf-8') as outputfile:\n",
    "    writer = csv.writer(outputfile, delimiter=delimiter, lineterminator='\\n')\n",
    "    for pair in extractSentencePairs(conversations):\n",
    "        writer.writerow(pair)\n",
    "print(\"\\nSample lines from file:\")\n",
    "printLines(datafile)\n",
    "\n"
   ]
  },
  {
   "source": [
    "# Load and trim data\n",
    "Now we have the data in the right format we can make all the necesary variables from the data, like the vocabulary and loading query/response pairs into memory\n",
    "\n",
    "Datafile will be the location of the dataset formated \"query \\t response\\n\" and turn it into a vocabulary"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "\n",
    "class Voc:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed  = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3 # Count SOS, EOS, PAD\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    # Remove words below a certain count threshold\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "\n",
    "        keep_words = []\n",
    "\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "\n",
    "        print('keep_words {} / {} = {:.4f}'.format(len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)))\n",
    "\n",
    "        # Reinitialize dictionaries\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3 # Count SOS, EOS, PAD\n",
    "\n",
    "        for word in keep_words:\n",
    "            self.addWord(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting preparing training data...\n",
      "Readin lines...\n",
      "Read 221282 sentence paris\n",
      "Trimmed to 64271 sentence paris\n",
      "Counting words...\n",
      "Counted words: 18008\n",
      "\n",
      "pairs:\n",
      "['there .', 'where ?']\n",
      "['you have my word . as a gentleman', 'you re sweet .']\n",
      "['hi .', 'looks like things worked out tonight huh ?']\n",
      "['you know chastity ?', 'i believe we share an art instructor']\n",
      "['have fun tonight ?', 'tons']\n",
      "['well no . . .', 'then that s all you had to say .']\n",
      "['then that s all you had to say .', 'but']\n",
      "['but', 'you always been this selfish ?']\n",
      "['do you listen to this crap ?', 'what crap ?']\n",
      "['what good stuff ?', 'the real you .']\n"
     ]
    }
   ],
   "source": [
    "MAX_length = 10 # Maximum sentence length to consider\n",
    "\n",
    "# Turn a Unicode string to plain ASCII\n",
    "def unicodeToASCII(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# Lowercase, trim, and remove non-leter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToASCII(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# Read query/response pairs and return a voc object\n",
    "def readVocs(datafile, corpus_name):\n",
    "    print(\"Readin lines...\")\n",
    "    # Read the file and split into lines\n",
    "    lines = open(datafile, encoding='utf-8').read().strip().split('\\n')\n",
    "    # Split every line into paris and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "    voc = Voc(corpus_name)\n",
    "    return voc, pairs\n",
    "\n",
    "\n",
    "#Returns True iff both sentences in a pair 'p' are under the MAX_LENGTH threshold\n",
    "def filterPair(p):\n",
    "    # INput sequences need to preserve the las word for EOS token\n",
    "    return len(p[0].split(\" \")) < MAX_length and len(p[1].split(\" \")) < MAX_length\n",
    "\n",
    "\n",
    "\n",
    "# Filter pairs using filterPair condition\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "\n",
    "# Using the functions defined above, return a populated voc object and paris list\n",
    "def laodPerpareData(corpus, corpus_name, datafile, save_dir):\n",
    "    print(\"Starting preparing training data...\")\n",
    "    voc, pairs = readVocs(datafile, corpus_name)\n",
    "    print(\"Read {!s} sentence paris\".format(len(pairs)))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to {!s} sentence paris\".format(len(pairs)))\n",
    "    print(\"Counting words...\")\n",
    "\n",
    "    for pair in pairs:\n",
    "        voc.addSentence(pair[0])\n",
    "        voc.addSentence(pair[1])\n",
    "    print(\"Counted words:\", voc.num_words)\n",
    "    return voc, pairs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "save_dir = os.path.join(\"assets\", \"save\")\n",
    "voc, pairs = laodPerpareData(corpus, corpus_name, datafile, save_dir)\n",
    "# Print some paris to validate\n",
    "print(\"\\npairs:\")\n",
    "for pair in pairs[:10]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "keep_words 7823 / 18005 = 0.4345\n",
      "Trimmed form 64271 pairs to 53165, 0.8272 of total\n"
     ]
    }
   ],
   "source": [
    "MIN_COUNT = 3 # Minimum word count threshold for trimming\n",
    "\n",
    "def trimRareWords(voc, pairs, MIN_COUNT):\n",
    "    # Trinm words used under the MIN_COUNT from the voc\n",
    "    voc.trim(MIN_COUNT)\n",
    "    # Filter out paris with trimmed words\n",
    "    keep_pairs = []\n",
    "    for pair in pairs:\n",
    "        input_sentence = pair[0]\n",
    "        output_sentence = pair[1]\n",
    "        keep_input = True\n",
    "        keep_output = True\n",
    "        # Check for input sentence\n",
    "        for word in input_sentence.split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep_input = False\n",
    "                break\n",
    "        # Check for output sentence\n",
    "        for word in output_sentence.split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep_output = False\n",
    "                break\n",
    "        \n",
    "        # Onlly keep pairs that do not contrain trrimmed word(s) in thier input or output sentence\n",
    "        if keep_input and keep_output:\n",
    "            keep_pairs.append(pair)\n",
    "    print(\"Trimmed form {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs),len(keep_pairs) / len(pairs)))\n",
    "    return keep_pairs\n",
    "\n",
    "\n",
    "# Trim voc and paris\n",
    "pairs = trimRareWords(voc, pairs, MIN_COUNT)"
   ]
  },
  {
   "source": [
    "# Preparing the data from the model\n",
    "This uses a different type of tensor as previously used in the other examples instead of one hot encoding whihc uses really long arrays to represent each word. This implementation takes the index from the vocab array to represent a word and a sentance is an array of index's.\n",
    "example\n",
    "``` Python\n",
    "vocab = [\"it\", \"nice\", \"you\", \"meet\", \"to\", \"is\"]\n",
    "input_tensor = [0, 5, 1, 4 ,3, 2] # \"it is nice to meet you\" --> tensor\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input_variable: tensor([[  76,  124,   33,    7,  625],\n        [  37,    4,  100,  349,    4],\n        [ 175,   12,  197,    4,    2],\n        [  76,  780,  117,    2,    0],\n        [2304,  180,  211,    0,    0],\n        [   4,   25,  443,    0,    0],\n        [  62,   63,   76,    0,    0],\n        [   4,    4,    4,    0,    0],\n        [   2,    2,    2,    0,    0]], device='cuda:0')\nlengths: tensor([9, 9, 9, 4, 3], device='cuda:0')\ntarget_variable: tensor([[  25,  318,  572,   25, 2034],\n        [ 197,  842,    6,  200,  359],\n        [ 117,  387,    2,   64,  211],\n        [ 118,    4,    0,  693,   22],\n        [  40,    2,    0,    7,    4],\n        [  24,    0,    0,   14,    2],\n        [   4,    0,    0, 1538,    0],\n        [   2,    0,    0,    4,    0],\n        [   0,    0,    0,    2,    0]], device='cuda:0')\nmask: tensor([[ True,  True,  True,  True,  True],\n        [ True,  True,  True,  True,  True],\n        [ True,  True,  True,  True,  True],\n        [ True,  True, False,  True,  True],\n        [ True,  True, False,  True,  True],\n        [ True, False, False,  True,  True],\n        [ True, False, False,  True, False],\n        [ True, False, False,  True, False],\n        [False, False, False,  True, False]], device='cuda:0')\nmax_target_len: 9\n"
     ]
    }
   ],
   "source": [
    "def indexesFromSentence(voc, sentence):\n",
    "    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
    "\n",
    "\n",
    "def zeroPadding(l, fillvalue=PAD_token):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "\n",
    "def binaryMatrix(l, value=PAD_token):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == PAD_token:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m\n",
    "\n",
    "\n",
    "# Returns padded input sequence tensor and lengths\n",
    "def inputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch]).cuda()\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    padVar = torch.LongTensor(padList).cuda()\n",
    "    return padVar, lengths\n",
    "\n",
    "\n",
    "# Returns padded target sequence tensor, padding mask, and max target length\n",
    "def outputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    mask = binaryMatrix(padList)\n",
    "    mask = torch.BoolTensor(mask).cuda()\n",
    "    padVar = torch.LongTensor(padList).cuda()\n",
    "    return padVar, mask, max_target_len\n",
    "\n",
    "\n",
    "\n",
    "def batch2TrainData(voc, pair_batch):\n",
    "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "    input_batch, output_batch = [], []\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "    inp, lengths = inputVar(input_batch, voc)\n",
    "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
    "    return inp, lengths, output, mask, max_target_len\n",
    "\n",
    "\n",
    "# Example for validation\n",
    "small_batch_size = 5\n",
    "batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
    "\n",
    "print(\"input_variable:\", input_variable)\n",
    "print(\"lengths:\", lengths)\n",
    "print(\"target_variable:\", target_variable)\n",
    "print(\"mask:\", mask)\n",
    "print(\"max_target_len:\", max_target_len)"
   ]
  },
  {
   "source": [
    "# The model\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "\n",
    "        # Initialize GRU\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        # Convert word indexes to embeddings\n",
    "        embedded = self.embedding(input_seq)\n",
    "        # Pack padded batch of sequences for RNN module\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        # Forward pass through GRU\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        # Unpack padding\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        # Sum bidirectional GRU outputs\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:,:,self.hidden_size:]\n",
    "        # Return output and final hidden state\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :oung attention layer\n",
    "class Attn(nn.Module):\n",
    "    def __init__ (self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        if self.method not in ['dot','general','concat']:\n",
    "            raise ValueError(self.method, \"is not an apropriate attention method.\")\n",
    "        self.hidden_size = hidden_size\n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size).cuda()\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size).cuda()\n",
    "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "\n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "\n",
    "    def general_score(self, hidden, encoder_output):\n",
    "        energy= self.attn(encoder_output)\n",
    "        return torch.sum(hidden * energy, dim=2)\n",
    "\n",
    "    def cocat_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
    "        return torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, hidden, encoder_output):\n",
    "        # Calculate attention weights (energies) based on the given method\n",
    "        if self.method == 'general':\n",
    "            attn_energies = self.general_score(hidden, encoder_output)\n",
    "        elif self.method == 'concat':\n",
    "            attn_energies = self.concat_score(hidden, encoder_output)\n",
    "        elif self.method == 'dot':\n",
    "            attn_energies = self.dot_score(hidden, encoder_output)\n",
    "\n",
    "            # Transpose max_length and batch_size dimensions\n",
    "            attn_energies = attn_energies.t()\n",
    "\n",
    "            # Return the softmax normalized probability scores (with added dimension)\n",
    "            return F.softmax(attn_energies, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = embedding.cuda()\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size).cuda()\n",
    "        self.out = nn.Linear(hidden_size, output_size).cuda()\n",
    "\n",
    "\n",
    "        self. attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "\n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0,1))\n",
    "\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskNLLLoss(inp, target, mask):\n",
    "    nTotal = mask.sum()\n",
    "    corssEntropy = -torch.log(torch.gather(inp, 1, target.view(-1,1)).squeeze(1))\n",
    "    loss = corssEntropy.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_length):\n",
    "\n",
    "    # Zero gradiants\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Set device options\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "\n",
    "    # Forward pass through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "    # Create initial deccoder input (start with SOS tokens for each sentence)\n",
    "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "\n",
    "    # Set initial decoder hidden state to the encdoer's final hidden state\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "\n",
    "    # Determine if we are using teacher forcing this iteration\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "    # Forward batch of sequences thorugh decoder one time step at a time\n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # Teacher forcing: next input is current target\n",
    "            decodrer_input = target_variable[t].view(1,-1)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden,encoder_outputs)\n",
    "            # No teacher forcing: nect input is decoder's own current output\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "\n",
    "    # Perform backpropigation\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip gradiants: gradiants are modified in place\n",
    "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    # Adjust model weights\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return sum(print_losses) / n_totals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(mode_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename):\n",
    "\n",
    "    # Load batches for each iteration\n",
    "    training_batches = [batch2TrainData(voc, [random.choice(pairs) for  _ in range(batch_size)]) for _ in range(n_iteration)]\n",
    "\n",
    "    # Initializations\n",
    "    print(\"Initializing... \")\n",
    "    start_iteration = 1\n",
    "    print_loss = 0\n",
    "    if loadFilename:\n",
    "        start_iteration = checkpoint['iteration'] + 1\n",
    "    \n",
    "    # Traning loop\n",
    "    print (\"Training...\")\n",
    "    for iteration in range(start_iteration, n_iteration + 1):\n",
    "        training_batch = training_batches[iteration - 1]\n",
    "        # Extract fields from batch\n",
    "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "\n",
    "        # Run training iteration with batch\n",
    "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
    "        print_loss += loss\n",
    "\n",
    "        # Print progress\n",
    "        if iteration % print_every == 0:\n",
    "            print_loss_avg = print_loss / print_every\n",
    "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss:{:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
    "            print_loss = 0\n",
    "\n",
    "        # Save checkpoint\n",
    "        if iteration % save_every == 0:\n",
    "            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            torch.save({\n",
    "                'iteration': iteration,\n",
    "                'en': encoder.state_dict(),\n",
    "                'de': decoder.state_dict(),\n",
    "                'en_opt': encoder_optimizer.state_dict(),\n",
    "                'de_opt': decoder_optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'voc_dict': voc.__dict__,\n",
    "                'embedding': embedding.state_dict()\n",
    "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedySearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        # Forward input through encoder model\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        # Prepare encdoer's  final hidden layer to be firsst hidden input to the decoder\n",
    "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "        # Initialize decoder input with SOS_token\n",
    "        decoder_input = torch.ones(1,1, device=device, dtype=torch.long) * SOS_token\n",
    "        # Initialize tensor to append decoder words to\n",
    "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=device)\n",
    "        # Iteratively decode one word token at a time\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass through decoder\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            # Obtain most likely word token and its softmax score\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "            # Record token and score\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "            # Prepare current roken to be next decoder input (add a dimension)\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "        # Return collections of word tokens and scores\n",
    "        return all_tokens, all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate (encoder, decoder, searcher, voc, sentence, max_length=MAX_length):\n",
    "    ### Format input senttence as a batch\n",
    "    # words -> indexes\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
    "    # Create lengths tensor\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    # Transpose dimensions of batch to match models' expectations\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0,1)\n",
    "    # Use apropiate device\n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    # Decode sentence with searcher\n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    # indexes -> words\n",
    "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
    "    return decoded_words\n",
    "\n",
    "def evaluateInput(encoder, decoder, searcher, voc):\n",
    "    input_sentence =''\n",
    "    while(1):\n",
    "        try:\n",
    "            # Get input sentence\n",
    "            input_sentence = input('> ')\n",
    "            #Check if it is quit case\n",
    "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
    "            # Normalize sentence\n",
    "            input_sentence = normalizeString(input_sentence)\n",
    "            # Evaluate sentence\n",
    "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "            # Format and prinput response sentence\n",
    "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "            print ('Bot: ', ''.join(output_words))\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Error: Encoutered unknow word.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Bulding encoder and decoder...\n",
      "Models built and ready to go!!\n"
     ]
    }
   ],
   "source": [
    "# Configure models\n",
    "model_name = 'cb_model'\n",
    "attn_model = 'dot'\n",
    "hidden_size = 500\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 64\n",
    "\n",
    "# Set checkpoint to load from; set to None if starting from scratch\n",
    "loadFileName = None\n",
    "checkpoint_iter = 10000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Bulding encoder and decoder...\")\n",
    "# Initialize word embeddings\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "\n",
    "# Initialize encoder and decoder models\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
    "\n",
    "\n",
    "embedding = embedding.to(device)\n",
    "# User appropriate device\n",
    "encoder = encoder.to(device)\n",
    "decider = decoder.to(device)\n",
    "print('Models built and ready to go!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "lete: 92.0%; Average loss:4.6694\n",
      "Iteration: 3679; Percent complete: 92.0%; Average loss:4.4483\n",
      "Iteration: 3680; Percent complete: 92.0%; Average loss:4.4686\n",
      "Iteration: 3681; Percent complete: 92.0%; Average loss:4.6461\n",
      "Iteration: 3682; Percent complete: 92.0%; Average loss:4.5972\n",
      "Iteration: 3683; Percent complete: 92.1%; Average loss:4.5214\n",
      "Iteration: 3684; Percent complete: 92.1%; Average loss:4.5977\n",
      "Iteration: 3685; Percent complete: 92.1%; Average loss:4.6744\n",
      "Iteration: 3686; Percent complete: 92.2%; Average loss:4.5000\n",
      "Iteration: 3687; Percent complete: 92.2%; Average loss:4.3725\n",
      "Iteration: 3688; Percent complete: 92.2%; Average loss:4.4811\n",
      "Iteration: 3689; Percent complete: 92.2%; Average loss:4.5472\n",
      "Iteration: 3690; Percent complete: 92.2%; Average loss:4.7890\n",
      "Iteration: 3691; Percent complete: 92.3%; Average loss:4.5333\n",
      "Iteration: 3692; Percent complete: 92.3%; Average loss:4.3725\n",
      "Iteration: 3693; Percent complete: 92.3%; Average loss:4.7656\n",
      "Iteration: 3694; Percent complete: 92.3%; Average loss:4.5152\n",
      "Iteration: 3695; Percent complete: 92.4%; Average loss:4.6085\n",
      "Iteration: 3696; Percent complete: 92.4%; Average loss:4.4182\n",
      "Iteration: 3697; Percent complete: 92.4%; Average loss:4.4248\n",
      "Iteration: 3698; Percent complete: 92.5%; Average loss:4.5196\n",
      "Iteration: 3699; Percent complete: 92.5%; Average loss:4.5316\n",
      "Iteration: 3700; Percent complete: 92.5%; Average loss:4.4099\n",
      "Iteration: 3701; Percent complete: 92.5%; Average loss:4.4840\n",
      "Iteration: 3702; Percent complete: 92.5%; Average loss:4.4368\n",
      "Iteration: 3703; Percent complete: 92.6%; Average loss:4.3758\n",
      "Iteration: 3704; Percent complete: 92.6%; Average loss:4.6101\n",
      "Iteration: 3705; Percent complete: 92.6%; Average loss:4.3967\n",
      "Iteration: 3706; Percent complete: 92.7%; Average loss:4.4673\n",
      "Iteration: 3707; Percent complete: 92.7%; Average loss:4.6489\n",
      "Iteration: 3708; Percent complete: 92.7%; Average loss:4.4983\n",
      "Iteration: 3709; Percent complete: 92.7%; Average loss:4.5470\n",
      "Iteration: 3710; Percent complete: 92.8%; Average loss:4.5424\n",
      "Iteration: 3711; Percent complete: 92.8%; Average loss:4.4767\n",
      "Iteration: 3712; Percent complete: 92.8%; Average loss:4.6059\n",
      "Iteration: 3713; Percent complete: 92.8%; Average loss:4.4013\n",
      "Iteration: 3714; Percent complete: 92.8%; Average loss:4.6432\n",
      "Iteration: 3715; Percent complete: 92.9%; Average loss:4.5908\n",
      "Iteration: 3716; Percent complete: 92.9%; Average loss:4.4657\n",
      "Iteration: 3717; Percent complete: 92.9%; Average loss:4.5171\n",
      "Iteration: 3718; Percent complete: 93.0%; Average loss:4.7170\n",
      "Iteration: 3719; Percent complete: 93.0%; Average loss:4.4162\n",
      "Iteration: 3720; Percent complete: 93.0%; Average loss:4.3445\n",
      "Iteration: 3721; Percent complete: 93.0%; Average loss:4.5020\n",
      "Iteration: 3722; Percent complete: 93.0%; Average loss:4.6924\n",
      "Iteration: 3723; Percent complete: 93.1%; Average loss:4.2850\n",
      "Iteration: 3724; Percent complete: 93.1%; Average loss:4.4681\n",
      "Iteration: 3725; Percent complete: 93.1%; Average loss:4.4044\n",
      "Iteration: 3726; Percent complete: 93.2%; Average loss:4.5744\n",
      "Iteration: 3727; Percent complete: 93.2%; Average loss:4.5516\n",
      "Iteration: 3728; Percent complete: 93.2%; Average loss:4.4675\n",
      "Iteration: 3729; Percent complete: 93.2%; Average loss:4.5195\n",
      "Iteration: 3730; Percent complete: 93.2%; Average loss:4.2663\n",
      "Iteration: 3731; Percent complete: 93.3%; Average loss:4.3549\n",
      "Iteration: 3732; Percent complete: 93.3%; Average loss:4.5210\n",
      "Iteration: 3733; Percent complete: 93.3%; Average loss:4.6459\n",
      "Iteration: 3734; Percent complete: 93.3%; Average loss:4.6360\n",
      "Iteration: 3735; Percent complete: 93.4%; Average loss:4.6071\n",
      "Iteration: 3736; Percent complete: 93.4%; Average loss:4.6230\n",
      "Iteration: 3737; Percent complete: 93.4%; Average loss:4.6071\n",
      "Iteration: 3738; Percent complete: 93.5%; Average loss:4.4403\n",
      "Iteration: 3739; Percent complete: 93.5%; Average loss:4.6469\n",
      "Iteration: 3740; Percent complete: 93.5%; Average loss:4.4418\n",
      "Iteration: 3741; Percent complete: 93.5%; Average loss:4.4866\n",
      "Iteration: 3742; Percent complete: 93.5%; Average loss:4.5207\n",
      "Iteration: 3743; Percent complete: 93.6%; Average loss:4.5038\n",
      "Iteration: 3744; Percent complete: 93.6%; Average loss:4.6739\n",
      "Iteration: 3745; Percent complete: 93.6%; Average loss:4.4423\n",
      "Iteration: 3746; Percent complete: 93.7%; Average loss:4.4667\n",
      "Iteration: 3747; Percent complete: 93.7%; Average loss:4.4541\n",
      "Iteration: 3748; Percent complete: 93.7%; Average loss:4.5344\n",
      "Iteration: 3749; Percent complete: 93.7%; Average loss:4.6516\n",
      "Iteration: 3750; Percent complete: 93.8%; Average loss:4.6247\n",
      "Iteration: 3751; Percent complete: 93.8%; Average loss:4.7086\n",
      "Iteration: 3752; Percent complete: 93.8%; Average loss:4.5580\n",
      "Iteration: 3753; Percent complete: 93.8%; Average loss:4.4418\n",
      "Iteration: 3754; Percent complete: 93.8%; Average loss:4.4142\n",
      "Iteration: 3755; Percent complete: 93.9%; Average loss:4.5023\n",
      "Iteration: 3756; Percent complete: 93.9%; Average loss:4.4732\n",
      "Iteration: 3757; Percent complete: 93.9%; Average loss:4.4633\n",
      "Iteration: 3758; Percent complete: 94.0%; Average loss:4.7348\n",
      "Iteration: 3759; Percent complete: 94.0%; Average loss:4.6429\n",
      "Iteration: 3760; Percent complete: 94.0%; Average loss:4.3872\n",
      "Iteration: 3761; Percent complete: 94.0%; Average loss:4.7144\n",
      "Iteration: 3762; Percent complete: 94.0%; Average loss:4.4073\n",
      "Iteration: 3763; Percent complete: 94.1%; Average loss:4.4840\n",
      "Iteration: 3764; Percent complete: 94.1%; Average loss:4.4182\n",
      "Iteration: 3765; Percent complete: 94.1%; Average loss:4.6093\n",
      "Iteration: 3766; Percent complete: 94.2%; Average loss:4.3737\n",
      "Iteration: 3767; Percent complete: 94.2%; Average loss:4.5601\n",
      "Iteration: 3768; Percent complete: 94.2%; Average loss:4.4804\n",
      "Iteration: 3769; Percent complete: 94.2%; Average loss:4.6740\n",
      "Iteration: 3770; Percent complete: 94.2%; Average loss:4.2510\n",
      "Iteration: 3771; Percent complete: 94.3%; Average loss:4.5976\n",
      "Iteration: 3772; Percent complete: 94.3%; Average loss:4.5361\n",
      "Iteration: 3773; Percent complete: 94.3%; Average loss:4.5633\n",
      "Iteration: 3774; Percent complete: 94.3%; Average loss:4.5083\n",
      "Iteration: 3775; Percent complete: 94.4%; Average loss:4.5305\n",
      "Iteration: 3776; Percent complete: 94.4%; Average loss:4.4575\n",
      "Iteration: 3777; Percent complete: 94.4%; Average loss:4.3685\n",
      "Iteration: 3778; Percent complete: 94.5%; Average loss:4.4750\n",
      "Iteration: 3779; Percent complete: 94.5%; Average loss:4.5191\n",
      "Iteration: 3780; Percent complete: 94.5%; Average loss:4.3909\n",
      "Iteration: 3781; Percent complete: 94.5%; Average loss:4.3912\n",
      "Iteration: 3782; Percent complete: 94.5%; Average loss:4.4025\n",
      "Iteration: 3783; Percent complete: 94.6%; Average loss:4.5736\n",
      "Iteration: 3784; Percent complete: 94.6%; Average loss:4.5194\n",
      "Iteration: 3785; Percent complete: 94.6%; Average loss:4.4166\n",
      "Iteration: 3786; Percent complete: 94.7%; Average loss:4.5243\n",
      "Iteration: 3787; Percent complete: 94.7%; Average loss:4.4765\n",
      "Iteration: 3788; Percent complete: 94.7%; Average loss:4.6999\n",
      "Iteration: 3789; Percent complete: 94.7%; Average loss:4.4645\n",
      "Iteration: 3790; Percent complete: 94.8%; Average loss:4.6915\n",
      "Iteration: 3791; Percent complete: 94.8%; Average loss:4.5331\n",
      "Iteration: 3792; Percent complete: 94.8%; Average loss:4.4442\n",
      "Iteration: 3793; Percent complete: 94.8%; Average loss:4.4253\n",
      "Iteration: 3794; Percent complete: 94.8%; Average loss:4.7308\n",
      "Iteration: 3795; Percent complete: 94.9%; Average loss:4.5822\n",
      "Iteration: 3796; Percent complete: 94.9%; Average loss:4.3681\n",
      "Iteration: 3797; Percent complete: 94.9%; Average loss:4.5527\n",
      "Iteration: 3798; Percent complete: 95.0%; Average loss:4.4698\n",
      "Iteration: 3799; Percent complete: 95.0%; Average loss:4.4640\n",
      "Iteration: 3800; Percent complete: 95.0%; Average loss:4.4228\n",
      "Iteration: 3801; Percent complete: 95.0%; Average loss:4.5223\n",
      "Iteration: 3802; Percent complete: 95.0%; Average loss:4.4717\n",
      "Iteration: 3803; Percent complete: 95.1%; Average loss:4.4186\n",
      "Iteration: 3804; Percent complete: 95.1%; Average loss:4.3812\n",
      "Iteration: 3805; Percent complete: 95.1%; Average loss:4.5323\n",
      "Iteration: 3806; Percent complete: 95.2%; Average loss:4.5923\n",
      "Iteration: 3807; Percent complete: 95.2%; Average loss:4.6066\n",
      "Iteration: 3808; Percent complete: 95.2%; Average loss:4.5457\n",
      "Iteration: 3809; Percent complete: 95.2%; Average loss:4.5785\n",
      "Iteration: 3810; Percent complete: 95.2%; Average loss:4.5343\n",
      "Iteration: 3811; Percent complete: 95.3%; Average loss:4.6444\n",
      "Iteration: 3812; Percent complete: 95.3%; Average loss:4.6363\n",
      "Iteration: 3813; Percent complete: 95.3%; Average loss:4.6715\n",
      "Iteration: 3814; Percent complete: 95.3%; Average loss:4.3402\n",
      "Iteration: 3815; Percent complete: 95.4%; Average loss:4.5677\n",
      "Iteration: 3816; Percent complete: 95.4%; Average loss:4.6018\n",
      "Iteration: 3817; Percent complete: 95.4%; Average loss:4.3766\n",
      "Iteration: 3818; Percent complete: 95.5%; Average loss:4.6032\n",
      "Iteration: 3819; Percent complete: 95.5%; Average loss:4.3787\n",
      "Iteration: 3820; Percent complete: 95.5%; Average loss:4.5757\n",
      "Iteration: 3821; Percent complete: 95.5%; Average loss:4.4702\n",
      "Iteration: 3822; Percent complete: 95.5%; Average loss:4.5158\n",
      "Iteration: 3823; Percent complete: 95.6%; Average loss:4.6872\n",
      "Iteration: 3824; Percent complete: 95.6%; Average loss:4.3840\n",
      "Iteration: 3825; Percent complete: 95.6%; Average loss:4.5836\n",
      "Iteration: 3826; Percent complete: 95.7%; Average loss:4.3447\n",
      "Iteration: 3827; Percent complete: 95.7%; Average loss:4.4873\n",
      "Iteration: 3828; Percent complete: 95.7%; Average loss:4.6172\n",
      "Iteration: 3829; Percent complete: 95.7%; Average loss:4.5280\n",
      "Iteration: 3830; Percent complete: 95.8%; Average loss:4.7571\n",
      "Iteration: 3831; Percent complete: 95.8%; Average loss:4.3412\n",
      "Iteration: 3832; Percent complete: 95.8%; Average loss:4.3011\n",
      "Iteration: 3833; Percent complete: 95.8%; Average loss:4.5019\n",
      "Iteration: 3834; Percent complete: 95.9%; Average loss:4.2589\n",
      "Iteration: 3835; Percent complete: 95.9%; Average loss:4.4888\n",
      "Iteration: 3836; Percent complete: 95.9%; Average loss:4.4353\n",
      "Iteration: 3837; Percent complete: 95.9%; Average loss:4.4825\n",
      "Iteration: 3838; Percent complete: 96.0%; Average loss:4.4524\n",
      "Iteration: 3839; Percent complete: 96.0%; Average loss:4.2694\n",
      "Iteration: 3840; Percent complete: 96.0%; Average loss:4.6810\n",
      "Iteration: 3841; Percent complete: 96.0%; Average loss:4.3959\n",
      "Iteration: 3842; Percent complete: 96.0%; Average loss:4.4378\n",
      "Iteration: 3843; Percent complete: 96.1%; Average loss:4.5823\n",
      "Iteration: 3844; Percent complete: 96.1%; Average loss:4.5669\n",
      "Iteration: 3845; Percent complete: 96.1%; Average loss:4.5527\n",
      "Iteration: 3846; Percent complete: 96.2%; Average loss:4.3657\n",
      "Iteration: 3847; Percent complete: 96.2%; Average loss:4.5001\n",
      "Iteration: 3848; Percent complete: 96.2%; Average loss:4.3519\n",
      "Iteration: 3849; Percent complete: 96.2%; Average loss:4.3932\n",
      "Iteration: 3850; Percent complete: 96.2%; Average loss:4.4318\n",
      "Iteration: 3851; Percent complete: 96.3%; Average loss:4.6518\n",
      "Iteration: 3852; Percent complete: 96.3%; Average loss:4.5015\n",
      "Iteration: 3853; Percent complete: 96.3%; Average loss:4.5622\n",
      "Iteration: 3854; Percent complete: 96.4%; Average loss:4.4215\n",
      "Iteration: 3855; Percent complete: 96.4%; Average loss:4.6661\n",
      "Iteration: 3856; Percent complete: 96.4%; Average loss:4.5640\n",
      "Iteration: 3857; Percent complete: 96.4%; Average loss:4.5473\n",
      "Iteration: 3858; Percent complete: 96.5%; Average loss:4.4869\n",
      "Iteration: 3859; Percent complete: 96.5%; Average loss:4.5289\n",
      "Iteration: 3860; Percent complete: 96.5%; Average loss:4.3172\n",
      "Iteration: 3861; Percent complete: 96.5%; Average loss:4.4240\n",
      "Iteration: 3862; Percent complete: 96.5%; Average loss:4.5542\n",
      "Iteration: 3863; Percent complete: 96.6%; Average loss:4.4532\n",
      "Iteration: 3864; Percent complete: 96.6%; Average loss:4.5250\n",
      "Iteration: 3865; Percent complete: 96.6%; Average loss:4.5428\n",
      "Iteration: 3866; Percent complete: 96.7%; Average loss:4.4684\n",
      "Iteration: 3867; Percent complete: 96.7%; Average loss:4.6487\n",
      "Iteration: 3868; Percent complete: 96.7%; Average loss:4.4228\n",
      "Iteration: 3869; Percent complete: 96.7%; Average loss:4.5882\n",
      "Iteration: 3870; Percent complete: 96.8%; Average loss:4.5234\n",
      "Iteration: 3871; Percent complete: 96.8%; Average loss:4.3919\n",
      "Iteration: 3872; Percent complete: 96.8%; Average loss:4.8055\n",
      "Iteration: 3873; Percent complete: 96.8%; Average loss:4.4167\n",
      "Iteration: 3874; Percent complete: 96.9%; Average loss:4.3562\n",
      "Iteration: 3875; Percent complete: 96.9%; Average loss:4.5754\n",
      "Iteration: 3876; Percent complete: 96.9%; Average loss:4.6376\n",
      "Iteration: 3877; Percent complete: 96.9%; Average loss:4.5265\n",
      "Iteration: 3878; Percent complete: 97.0%; Average loss:4.4934\n",
      "Iteration: 3879; Percent complete: 97.0%; Average loss:4.5726\n",
      "Iteration: 3880; Percent complete: 97.0%; Average loss:4.6709\n",
      "Iteration: 3881; Percent complete: 97.0%; Average loss:4.2613\n",
      "Iteration: 3882; Percent complete: 97.0%; Average loss:4.6335\n",
      "Iteration: 3883; Percent complete: 97.1%; Average loss:4.1934\n",
      "Iteration: 3884; Percent complete: 97.1%; Average loss:4.4467\n",
      "Iteration: 3885; Percent complete: 97.1%; Average loss:4.3391\n",
      "Iteration: 3886; Percent complete: 97.2%; Average loss:4.7567\n",
      "Iteration: 3887; Percent complete: 97.2%; Average loss:4.4419\n",
      "Iteration: 3888; Percent complete: 97.2%; Average loss:4.5545\n",
      "Iteration: 3889; Percent complete: 97.2%; Average loss:4.3690\n",
      "Iteration: 3890; Percent complete: 97.2%; Average loss:4.6483\n",
      "Iteration: 3891; Percent complete: 97.3%; Average loss:4.7592\n",
      "Iteration: 3892; Percent complete: 97.3%; Average loss:4.5364\n",
      "Iteration: 3893; Percent complete: 97.3%; Average loss:4.3965\n",
      "Iteration: 3894; Percent complete: 97.4%; Average loss:4.4441\n",
      "Iteration: 3895; Percent complete: 97.4%; Average loss:4.5311\n",
      "Iteration: 3896; Percent complete: 97.4%; Average loss:4.5050\n",
      "Iteration: 3897; Percent complete: 97.4%; Average loss:4.6224\n",
      "Iteration: 3898; Percent complete: 97.5%; Average loss:4.3751\n",
      "Iteration: 3899; Percent complete: 97.5%; Average loss:4.5217\n",
      "Iteration: 3900; Percent complete: 97.5%; Average loss:4.5683\n",
      "Iteration: 3901; Percent complete: 97.5%; Average loss:4.5565\n",
      "Iteration: 3902; Percent complete: 97.5%; Average loss:4.3496\n",
      "Iteration: 3903; Percent complete: 97.6%; Average loss:4.3210\n",
      "Iteration: 3904; Percent complete: 97.6%; Average loss:4.4048\n",
      "Iteration: 3905; Percent complete: 97.6%; Average loss:4.4717\n",
      "Iteration: 3906; Percent complete: 97.7%; Average loss:4.4110\n",
      "Iteration: 3907; Percent complete: 97.7%; Average loss:4.5039\n",
      "Iteration: 3908; Percent complete: 97.7%; Average loss:4.4084\n",
      "Iteration: 3909; Percent complete: 97.7%; Average loss:4.5475\n",
      "Iteration: 3910; Percent complete: 97.8%; Average loss:4.5264\n",
      "Iteration: 3911; Percent complete: 97.8%; Average loss:4.6668\n",
      "Iteration: 3912; Percent complete: 97.8%; Average loss:4.3536\n",
      "Iteration: 3913; Percent complete: 97.8%; Average loss:4.4337\n",
      "Iteration: 3914; Percent complete: 97.9%; Average loss:4.5914\n",
      "Iteration: 3915; Percent complete: 97.9%; Average loss:4.5235\n",
      "Iteration: 3916; Percent complete: 97.9%; Average loss:4.5471\n",
      "Iteration: 3917; Percent complete: 97.9%; Average loss:4.5742\n",
      "Iteration: 3918; Percent complete: 98.0%; Average loss:4.8629\n",
      "Iteration: 3919; Percent complete: 98.0%; Average loss:4.6190\n",
      "Iteration: 3920; Percent complete: 98.0%; Average loss:4.4330\n",
      "Iteration: 3921; Percent complete: 98.0%; Average loss:4.5761\n",
      "Iteration: 3922; Percent complete: 98.0%; Average loss:4.6965\n",
      "Iteration: 3923; Percent complete: 98.1%; Average loss:4.3635\n",
      "Iteration: 3924; Percent complete: 98.1%; Average loss:4.3950\n",
      "Iteration: 3925; Percent complete: 98.1%; Average loss:4.4676\n",
      "Iteration: 3926; Percent complete: 98.2%; Average loss:4.4733\n",
      "Iteration: 3927; Percent complete: 98.2%; Average loss:4.5404\n",
      "Iteration: 3928; Percent complete: 98.2%; Average loss:4.4101\n",
      "Iteration: 3929; Percent complete: 98.2%; Average loss:4.4643\n",
      "Iteration: 3930; Percent complete: 98.2%; Average loss:4.4784\n",
      "Iteration: 3931; Percent complete: 98.3%; Average loss:4.5885\n",
      "Iteration: 3932; Percent complete: 98.3%; Average loss:4.5214\n",
      "Iteration: 3933; Percent complete: 98.3%; Average loss:4.5146\n",
      "Iteration: 3934; Percent complete: 98.4%; Average loss:4.4804\n",
      "Iteration: 3935; Percent complete: 98.4%; Average loss:4.4668\n",
      "Iteration: 3936; Percent complete: 98.4%; Average loss:4.3620\n",
      "Iteration: 3937; Percent complete: 98.4%; Average loss:4.5889\n",
      "Iteration: 3938; Percent complete: 98.5%; Average loss:4.5487\n",
      "Iteration: 3939; Percent complete: 98.5%; Average loss:4.5504\n",
      "Iteration: 3940; Percent complete: 98.5%; Average loss:4.2970\n",
      "Iteration: 3941; Percent complete: 98.5%; Average loss:4.4286\n",
      "Iteration: 3942; Percent complete: 98.6%; Average loss:4.5791\n",
      "Iteration: 3943; Percent complete: 98.6%; Average loss:4.4500\n",
      "Iteration: 3944; Percent complete: 98.6%; Average loss:4.7925\n",
      "Iteration: 3945; Percent complete: 98.6%; Average loss:4.1254\n",
      "Iteration: 3946; Percent complete: 98.7%; Average loss:4.6750\n",
      "Iteration: 3947; Percent complete: 98.7%; Average loss:4.4341\n",
      "Iteration: 3948; Percent complete: 98.7%; Average loss:4.5352\n",
      "Iteration: 3949; Percent complete: 98.7%; Average loss:4.5231\n",
      "Iteration: 3950; Percent complete: 98.8%; Average loss:4.4766\n",
      "Iteration: 3951; Percent complete: 98.8%; Average loss:4.7687\n",
      "Iteration: 3952; Percent complete: 98.8%; Average loss:4.3870\n",
      "Iteration: 3953; Percent complete: 98.8%; Average loss:4.6000\n",
      "Iteration: 3954; Percent complete: 98.9%; Average loss:4.5361\n",
      "Iteration: 3955; Percent complete: 98.9%; Average loss:4.3765\n",
      "Iteration: 3956; Percent complete: 98.9%; Average loss:4.6176\n",
      "Iteration: 3957; Percent complete: 98.9%; Average loss:4.6909\n",
      "Iteration: 3958; Percent complete: 99.0%; Average loss:4.6612\n",
      "Iteration: 3959; Percent complete: 99.0%; Average loss:4.5637\n",
      "Iteration: 3960; Percent complete: 99.0%; Average loss:4.4138\n",
      "Iteration: 3961; Percent complete: 99.0%; Average loss:4.5534\n",
      "Iteration: 3962; Percent complete: 99.1%; Average loss:4.4598\n",
      "Iteration: 3963; Percent complete: 99.1%; Average loss:4.7843\n",
      "Iteration: 3964; Percent complete: 99.1%; Average loss:4.5978\n",
      "Iteration: 3965; Percent complete: 99.1%; Average loss:4.4914\n",
      "Iteration: 3966; Percent complete: 99.2%; Average loss:4.6592\n",
      "Iteration: 3967; Percent complete: 99.2%; Average loss:4.4454\n",
      "Iteration: 3968; Percent complete: 99.2%; Average loss:4.4177\n",
      "Iteration: 3969; Percent complete: 99.2%; Average loss:4.4019\n",
      "Iteration: 3970; Percent complete: 99.2%; Average loss:4.5596\n",
      "Iteration: 3971; Percent complete: 99.3%; Average loss:4.4931\n",
      "Iteration: 3972; Percent complete: 99.3%; Average loss:4.4815\n",
      "Iteration: 3973; Percent complete: 99.3%; Average loss:4.1395\n",
      "Iteration: 3974; Percent complete: 99.4%; Average loss:4.4936\n",
      "Iteration: 3975; Percent complete: 99.4%; Average loss:4.3696\n",
      "Iteration: 3976; Percent complete: 99.4%; Average loss:4.5751\n",
      "Iteration: 3977; Percent complete: 99.4%; Average loss:4.5272\n",
      "Iteration: 3978; Percent complete: 99.5%; Average loss:4.5866\n",
      "Iteration: 3979; Percent complete: 99.5%; Average loss:4.2720\n",
      "Iteration: 3980; Percent complete: 99.5%; Average loss:4.5819\n",
      "Iteration: 3981; Percent complete: 99.5%; Average loss:4.4691\n",
      "Iteration: 3982; Percent complete: 99.6%; Average loss:4.5330\n",
      "Iteration: 3983; Percent complete: 99.6%; Average loss:4.5404\n",
      "Iteration: 3984; Percent complete: 99.6%; Average loss:4.7270\n",
      "Iteration: 3985; Percent complete: 99.6%; Average loss:4.5109\n",
      "Iteration: 3986; Percent complete: 99.7%; Average loss:4.2902\n",
      "Iteration: 3987; Percent complete: 99.7%; Average loss:4.4872\n",
      "Iteration: 3988; Percent complete: 99.7%; Average loss:4.3838\n",
      "Iteration: 3989; Percent complete: 99.7%; Average loss:4.6293\n",
      "Iteration: 3990; Percent complete: 99.8%; Average loss:4.5446\n",
      "Iteration: 3991; Percent complete: 99.8%; Average loss:4.2380\n",
      "Iteration: 3992; Percent complete: 99.8%; Average loss:4.4281\n",
      "Iteration: 3993; Percent complete: 99.8%; Average loss:4.4361\n",
      "Iteration: 3994; Percent complete: 99.9%; Average loss:4.5388\n",
      "Iteration: 3995; Percent complete: 99.9%; Average loss:4.5287\n",
      "Iteration: 3996; Percent complete: 99.9%; Average loss:4.5177\n",
      "Iteration: 3997; Percent complete: 99.9%; Average loss:4.2117\n",
      "Iteration: 3998; Percent complete: 100.0%; Average loss:4.4807\n",
      "Iteration: 3999; Percent complete: 100.0%; Average loss:4.7363\n",
      "Iteration: 4000; Percent complete: 100.0%; Average loss:4.5017\n"
     ]
    }
   ],
   "source": [
    "# Configure training/optimization\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 0.5\n",
    "n_iteration = 4000\n",
    "print_every = 1\n",
    "save_every = 500\n",
    "\n",
    "\n",
    "# Enssure dropout layers are in train mode\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "# Initialize optimizers\n",
    "print('Building optimizers...')\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "if loadFileName:\n",
    "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
    "    decoder_optimiser.load_state_dict(decoder_optimiser_sd)\n",
    "\n",
    "# If you cuda, configure cuda to call\n",
    "for state in encoder_optimizer.state.values():\n",
    "    for k,v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "\n",
    "for state in decoder_optimizer.state.values():\n",
    "    for k,v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "\n",
    "\n",
    "# Run training iterations\n",
    "print(\"Starting Training!\")\n",
    "trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer,decoder_optimizer,embedding,encoder_n_layers,decoder_n_layers,save_dir,n_iteration, batch_size, print_every, save_every, clip, corpus_name,loadFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Bot:  hi........\n",
      "Bot:  i.........\n",
      "Error: Encoutered unknow word.\n",
      "Bot:  youyouyou\n",
      "Bot:  is........\n",
      "Bot:  yes........\n",
      "Bot:  i.........\n",
      "Bot:  you.........\n",
      "Bot:  ii........\n",
      "Bot:  youyouyou\n",
      "Bot:  i.........\n",
      "Bot:  youyou..\n",
      "Bot:  i.\n",
      "Bot:  you.\n",
      "Bot:  you.\n",
      "Bot:  i........\n",
      "Bot:  is........\n",
      "Error: Encoutered unknow word.\n",
      "Bot:  i.........\n",
      "Bot:  i.........\n",
      "Bot:  what.\n",
      "Bot:  what.\n",
      "Bot:  youyou..\n",
      "Bot:  what..\n",
      "Bot:  what.\n",
      "Bot:  what.\n",
      "Bot:  i.........\n",
      "Bot:  is.......\n",
      "Bot:  what.\n",
      "Error: Encoutered unknow word.\n",
      "Bot:  i........\n",
      "Bot:  youyouyou.\n",
      "Bot:  hi........\n",
      "Bot:  hihi..\n",
      "Bot:  i.....\n",
      "Error: Encoutered unknow word.\n",
      "Bot:  whatyou\n",
      "Bot:  i.........\n",
      "Bot:  i..\n",
      "Bot:  i.........\n",
      "Bot:  i.\n",
      "Bot:  ii.\n",
      "Bot:  i.........\n",
      "Bot:  yes.........\n",
      "Bot:  i.........\n",
      "Bot:  you.you.\n",
      "Bot:  i.........\n",
      "Bot:  youyouyou.....\n",
      "Bot:  youyouyouyouyou\n",
      "Bot:  i.........\n",
      "Bot:  i.........\n"
     ]
    }
   ],
   "source": [
    "# Set dropout layers to eval mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "searcher = GreedySearchDecoder(encoder=encoder, decoder=decoder)\n",
    "\n",
    "# Begin chatting (uncomment and run the following line to begin)\n",
    "evaluateInput(encoder, decoder, searcher, voc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}