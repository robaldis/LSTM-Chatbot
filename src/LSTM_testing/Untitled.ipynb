{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, TimeDistributed, RepeatVector, Dense\n",
    "import numpy as np\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "import io\n",
    "import unicodedata\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init()\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data input\n",
    "# Use my own database\n",
    "path_to_file='assets/human-robot.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "      if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentance(w):\n",
    "  w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "  # creating a space between a word and the punctuation following it\n",
    "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "  w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "  w = w.strip()\n",
    "\n",
    "  # adding a start and an end token to the sentence\n",
    "  # so that the model know when to start and stop predicting.\n",
    "  w = '<start> ' + w + ' <end>'\n",
    "  return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-5-16f4380aa300>, line 32)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-16f4380aa300>\"\u001b[0;36m, line \u001b[0;32m32\u001b[0m\n\u001b[0;31m    if calc_argmax:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class CharacterTable (object):\n",
    "    \"\"\"Given a set of characters:\n",
    "    + Encode them to a one hot integer representation\n",
    "    + Decode the on hot integer representation to thier character output\n",
    "    + Decode a vector of probabilities to their character output\n",
    "    \"\"\"\n",
    "    def __init__(self, chars):\n",
    "        \"\"\"Initialize character table\n",
    "        # Arguments\n",
    "           chars: Characters that can appear in the input\n",
    "        \"\"\"\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indicies_char = dict((i, c) for i, c in enumerate(slef.chars))\n",
    "\n",
    "        \n",
    "\n",
    "    def encode(self, C, num_rows):\n",
    "        \"\"\"One hot encode given string C.\n",
    "        # Arguments\n",
    "            num_rows: Number of rows in the returned one hot encoding. This is \n",
    "                used to keep the ' of the rows for each data the same.\n",
    "        \"\"\"\n",
    "        x = np.zeros((num_rows, len(self.chars)))\n",
    "        for i, c in enumerate(c):\n",
    "            if calc_argmax:\n",
    "                x = x.argmax(axis=-1)\n",
    "            return ''.join(self.indicies_char[x] for x in x)\n",
    "\n",
    "\n",
    "        def decode(self, x, calc_argmax=True):\n",
    "        if calc_argmax:\n",
    "            x = x.argmax(axis=-1)\n",
    "        return ''.join(self.indices_char[x] for x in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'assets/human-robot.txt'\n",
    "lines = io.open(path, encoding='UTF-8').read().split('\\n')\n",
    "word_pairs = [[preprocess_sentance(w) for w in l.split('\\t')] for l in lines[:1000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_layout (object):\n",
    "    def __init__(self, path):\n",
    "        self.inp, self.targ = self.generate_word_pairs(path)\n",
    "\n",
    "        #Generate vocab list\n",
    "        self.inp_vocab = self.generate_vocab(self.inp)\n",
    "        self.targ_vocab = self.generate_vocab(self.targ)\n",
    "\n",
    "        self.input_features_dict = dict([(token, i) for i, token in enumerate(self.inp_vocab)])\n",
    "        self.reverse_input_features_dict = dict((i, token) for token, i in self.input_features_dict.items())\n",
    "\n",
    "        self.inp_max_length = len(max(self.inp, key=len))\n",
    "        self.targ_max_length = len(max(self.targ, key=len))\n",
    "\n",
    "        #Generate one hot encoded arrays\n",
    "        #for i, sentence in enumerate(self.inp):\n",
    "            #self.x = self.encode(sentence, self.inp_max_length, self.reverse_input_features_dict)\n",
    "\n",
    "\n",
    "    def generate_word_pairs(self, path):\n",
    "        # Split the dataset into two parts\n",
    "        lines = io.open(path, encoding='UTF-8').read().split('\\n')\n",
    "        word_pairs = [[preprocess_sentance(w) for w in l.split('\\t')] for l in lines[:1000]]\n",
    "        # seperate the word pairs out\n",
    "        return zip(*word_pairs)\n",
    "\n",
    "    def generate_vocab(self,array):\n",
    "        tokens = set()\n",
    "        for line in array:\n",
    "            for token in re.findall(r\"[\\w']+|[^\\s\\w]\", line):\n",
    "                if token not in tokens:\n",
    "                    tokens.add(token)\n",
    "        return tokens\n",
    "\n",
    "    \n",
    "\n",
    "    def encode (self, C, num_rows, vocab):\n",
    "        x = np.zeros((num_rows, len(vocab)))\n",
    "        for i, c in enumerate(C):\n",
    "            x[i, self.input_features_dict[c]] = 1\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1104\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "' '",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-623a5a0ddb8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minp_max_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minp_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minp_max_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse_input_features_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-aa9956f5d737>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, C, num_rows, vocab)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_features_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ' '"
     ]
    }
   ],
   "source": [
    "path_to_file='assets/human-robot.txt'\n",
    "dataset = Data_layout(path_to_file)\n",
    "print(dataset.inp_max_length)\n",
    "x = np.zeros((len(dataset.inp), dataset.inp_max_length, len(dataset.inp_vocab)), dtype=np.bool)\n",
    "for i, sentence in enumerate (dataset.inp):\n",
    "    x[i] = dataset.encode(sentence, dataset.inp_max_length, dataset.reverse_input_features_dict)\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.traning_size = 1000\n",
    "# config.digits = 5\n",
    "config.hidden_size = 128\n",
    "config.batch_size = 128\n",
    "\n",
    "x, y,  = load_dataset(path_to_file, 1000)\n",
    "\n",
    "# maxlen = config.digits + 1 + config.digits\n",
    "\n",
    "ctable = CharacterTable(chars)\n"
   ]
  },
  {
   "source": [
    "# Things needed to update the model to work with my data\n",
    "* chars: is the vocabulary of the input data\n",
    "* digits: the number of unique \"tokens\" are in the input text\n",
    "* maxlen: the longest possible length of a input vector\n",
    "* x: list of all input data one hot integer array\n",
    "* y: list of all output data one hot integer array"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# Needs to be modified\n",
    "mode.add(LSTM(config.bidden_size, input_shape=(maxlen, len(chars))))\n",
    "model.add(RepeatVector(config.digits+1))\n",
    "model.add(LSTM(config.hidden_size, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(len(chars), activation='softmax')))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(1, 200):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iterations', iteration)\n",
    "    model.fit(x_train, y_train, batch_size=config.batch_size=, epochs=1, validation_data=(x_val, y_val), callbacks=[WandbCallback()])\n",
    "    \n",
    "    # Select 10 sampless from the validation set at random so we can visualize errors\n",
    "    for i in range(10):\n",
    "        ind = np.random.randint(0, len(x_val))\n",
    "        rowx, rowy - x_val[np.array([ind])], y_val[np.array([ind])]\n",
    "        preds = model.predict_classes(rowx verbose=0)\n",
    "        q = ctable.decode(rowy[0])\n",
    "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
    "        print('Q', q, end=' ')\n",
    "        print('T', correct, end=' ')\n",
    "        if correct == guess:\n",
    "            print('☑', end=' ')\n",
    "        else:\n",
    "            print('☒', end=' ')\n",
    "        print(guess)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}